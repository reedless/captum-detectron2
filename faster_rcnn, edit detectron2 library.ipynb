{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f7ecb39",
   "metadata": {},
   "source": [
    "## Read sample COCO img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91544d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bb95c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if input image is in range 0..1, please first multiply img by 255\n",
    "# assume image is ndarray of shape [height, width, channels] where channels can be 1, 3 or 4\n",
    "def imshow(img):\n",
    "    _,ret = cv2.imencode('.jpg', img) \n",
    "    i = IPython.display.Image(data=ret)\n",
    "    IPython.display.display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d51c493",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('000000000001.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b41d7b",
   "metadata": {},
   "source": [
    "## Load same faster rcnn model as in VAS, run prediction on img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ca529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae656898",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "\n",
    "model = build_model(cfg).eval()\n",
    "\n",
    "DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "outputs = model(torch.from_numpy(img).permute(2,0,1).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29817ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reedless/detectron2/detectron2/modeling/meta_arch/rcnn.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"pixel_mean\", torch.tensor(pixel_mean).view(-1, 1, 1), False)\n",
      "/home/reedless/detectron2/detectron2/modeling/meta_arch/rcnn.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"pixel_std\", torch.tensor(pixel_std).view(-1, 1, 1), False)\n"
     ]
    }
   ],
   "source": [
    "from modified_rcnn import ModifiedGeneralizedRCNN\n",
    "device = 'cuda'\n",
    "modified = ModifiedGeneralizedRCNN(model).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d7c30dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "inference() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m DetectionCheckpointer(modified)\u001b[38;5;241m.\u001b[39mload(cfg\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mWEIGHTS)\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodified\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_postprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/captum-detectron2/modified_rcnn.py:123\u001b[0m, in \u001b[0;36mModifiedGeneralizedRCNN.inference\u001b[0;34m(self, batched_inputs, detected_instances, do_postprocess, class_scores_only)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproposals\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    121\u001b[0m         proposals \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproposals\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batched_inputs]\n\u001b[0;32m--> 123\u001b[0m     results, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroi_heads\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_scores_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     detected_instances \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m detected_instances]\n",
      "File \u001b[0;32m~/captum-detectron2/modified_standard_roi_heads.py:72\u001b[0m, in \u001b[0;36mModifiedStandardROIHeads.forward\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proposals, losses\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     pred_instances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_box\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_scores_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# During inference cascaded prediction is used: the mask and keypoints heads are only\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# applied to the top scoring box detections.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# pred_instances = self.forward_with_given_boxes(features, pred_instances)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pred_instances, {}\n",
      "File \u001b[0;32m~/captum-detectron2/modified_standard_roi_heads.py:114\u001b[0m, in \u001b[0;36mModifiedStandardROIHeads._forward_box\u001b[0;34m(self, features, proposals, class_scores_only)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m losses\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     pred_instances, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbox_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_scores_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pred_instances\n",
      "\u001b[0;31mTypeError\u001b[0m: inference() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "DetectionCheckpointer(modified).load(cfg.MODEL.WEIGHTS)\n",
    "results = modified.inference(torch.from_numpy(img).permute(2,0,1).unsqueeze(0), do_postprocess=False, class_scores_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21e97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8df44d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0cad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape, torch.from_numpy(img).permute(2,0,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac062673",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs[0]['instances'][0].pred_classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94376a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]['instances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d58cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]['instances'].pred_boxes.tensor.cpu().detach().int().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd2ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "clone = img.copy()\n",
    "\n",
    "for box in outputs[0]['instances'].pred_boxes.tensor.cpu().detach().int().numpy():\n",
    "    x0, y0, x1, y1 = box\n",
    "    clone[y0:y1, x0] = np.full((y1-y0, 3), 255)\n",
    "    clone[y0:y1, x1] = np.full((y1-y0, 3), 255)\n",
    "    clone[y0, x0:x1] = np.full((x1-x0, 3), 255)\n",
    "    clone[y1, x0:x1] = np.full((x1-x0, 3), 255)\n",
    "\n",
    "imshow(clone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db149a",
   "metadata": {},
   "source": [
    "### Visualise output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca546a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "v = Visualizer(img[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[0][\"instances\"].to(\"cpu\"))\n",
    "imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0230f4f3",
   "metadata": {},
   "source": [
    "## Partially executing faster rcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220150e1",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7216cf27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_img = torch.from_numpy(img).permute(2,0,1).unsqueeze(0).float().to(\"cuda\")\n",
    "\n",
    "images = model.preprocess_image(input_img)\n",
    "\n",
    "# features\n",
    "features = model.backbone(images.tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494d640",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a764262",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['p2'].shape, features['p6'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729cfb68",
   "metadata": {},
   "source": [
    "#### Global Average Pool feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e77fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.AvgPool3d(features['p2'].shape[1:])\n",
    "print(m(features['p2'])[0, 0, 0, 0])\n",
    "m = torch.nn.AvgPool3d(features['p6'].shape[1:])\n",
    "print(m(features['p6'])[0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8136223a",
   "metadata": {},
   "source": [
    "### Features to RPN proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56c100",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_objectness_logits, pred_anchor_deltas = model.proposal_generator.rpn_head([features[f] for f in cfg.MODEL.RPN.IN_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba45950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_objectness_logits[0].shape, pred_objectness_logits[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531865b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c3d952",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(2, 3))\n",
    "\n",
    "for i in range(len(pred_objectness_logits)):\n",
    "    heatmap = (pred_objectness_logits[i]\n",
    "               .permute(0, 2, 3, 1)\n",
    "               .cpu().detach()\n",
    "               .apply_(sigmoid)[0,:,:,1])\n",
    "                # 3 types of boxes, 1 is square, or 1:1 boxes\n",
    "    fig.add_subplot(2, 3, i+1)\n",
    "    plt.imshow(img)\n",
    "    scale = int(img.shape[0]/heatmap.shape[0])\n",
    "    plt.imshow(np.kron(heatmap, np.ones((scale, scale))), cmap=plt.get_cmap('jet'), alpha=0.4)\n",
    "    \n",
    "fig.set_size_inches(18, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc3fa5",
   "metadata": {},
   "source": [
    "### Proposal Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d793d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proposals\n",
    "proposals, losses = model.proposal_generator(images, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238af2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.proposal_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13555130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proposals[0] # proposal_boxes, objectness_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a5eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "objectness_scores = proposals[0].objectness_logits.cpu().apply_(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545fce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clone = np.zeros(img.shape[:2])\n",
    "boxes = proposals[0].proposal_boxes.tensor.cpu().detach().int().numpy()\n",
    "\n",
    "for i in range(len(boxes)):\n",
    "    box = boxes[i]\n",
    "    x0, y0, x1, y1 = box\n",
    "    if x1 == 640:\n",
    "        x1 -= 1\n",
    "    if y1 == 480:\n",
    "        y1 -= 1\n",
    "    clone[y0:y1, x0:x1] += np.full((y1-y0, x1-x0), objectness_scores[i])\n",
    "    \n",
    "plt.imshow(img)\n",
    "plt.imshow(clone, cmap=plt.get_cmap('jet'), alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6349753b",
   "metadata": {},
   "source": [
    "### ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef839ff3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "type(model.roi_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad327ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = [features[f] for f in model.roi_heads.box_in_features]\n",
    "model.roi_heads.box_in_features, features.keys() # note the missing 'p6' from features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "box_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae3c1e",
   "metadata": {},
   "source": [
    "#### Some examples of cropped ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd2b54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(1, 5))\n",
    "\n",
    "for i in range(5):\n",
    "    cropped_roi = torch.sum(box_features[0].cpu().detach(), dim=0)\n",
    "    fig.add_subplot(1, 5, i+1)\n",
    "    plt.imshow(cropped_roi)\n",
    "    \n",
    "fig.set_size_inches(10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c137f2",
   "metadata": {},
   "source": [
    "#### Flatten cropped ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.roi_heads.box_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_features2 = model.roi_heads.box_head(box_features)\n",
    "box_features2, box_features2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff82723",
   "metadata": {},
   "source": [
    "#### Convert flattened and linear layers to class scores and proposal deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ba353",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.roi_heads.box_predictor.cls_score.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807aea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, proposal_deltas = predictions = model.roi_heads.box_predictor(box_features2)\n",
    "scores.shape, proposal_deltas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810164a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_instances, _ = model.roi_heads.box_predictor.inference(predictions, proposals)\n",
    "pred_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6786c8f4",
   "metadata": {},
   "source": [
    "#### Break down model.roi_heads.box_predictor.inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4fe588",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "list[Tensor]:\n",
    "A list of Tensors of predicted class-specific or class-agnostic boxes\n",
    "for each image. Element i has shape (Ri, K * B) or (Ri, B), where Ri is\n",
    "the number of proposals for image i and B is the box dimension (4 or 5)\n",
    "'''\n",
    "# basically converts proposal_deltas to actual bounding boxes\n",
    "boxes = model.roi_heads.box_predictor.predict_boxes(predictions, proposals)\n",
    "boxes[0].shape # per ROI, we have 80 class-specific bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c98b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "list[Tensor]:\n",
    "A list of Tensors of predicted class probabilities for each image.\n",
    "Element i has shape (Ri, K + 1), where Ri is the number of proposals for image i.\n",
    "'''\n",
    "# basically F.softmax(scores, dim=-1) for all\n",
    "scores = model.roi_heads.box_predictor.predict_probs(predictions, proposals)\n",
    "scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb20ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(proposals) = len(image_shapes) = 1\n",
    "image_shapes = [x.image_size for x in proposals]\n",
    "image_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321101b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "def fast_rcnn_inference(\n",
    "    boxes: List[torch.Tensor],\n",
    "    scores: List[torch.Tensor],\n",
    "    image_shapes: List[Tuple[int, int]],\n",
    "    score_thresh: float, # set by config, is 0.5 rn\n",
    "    nms_thresh: float, # 0.5 by default\n",
    "    topk_per_image: int, # 100 by default, but usually won't have that many detected objects in an image\n",
    "):\n",
    "    \n",
    "    # result_per_image : list[(Instances, Tensor)]\n",
    "    result_per_image = [\n",
    "        fast_rcnn_inference_single_image(\n",
    "            boxes_per_image, scores_per_image, image_shape, score_thresh, nms_thresh, topk_per_image\n",
    "        )\n",
    "        for scores_per_image, boxes_per_image, image_shape in zip(scores, boxes, image_shapes)\n",
    "    ]\n",
    "    \n",
    "    # list[Instances], list[Tensor]\n",
    "    return [x[0] for x in result_per_image], [x[1] for x in result_per_image]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979ff0c",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "Edit fast_rcnn_inference_single_image on jpnote to return array of instances with associated class probabilities DONE\n",
    "\n",
    "Copy changes to subclass of detectron2.modeling.roi_heads.fast_rcnn.fast_rcnn_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be82ede",
   "metadata": {},
   "source": [
    "### Think...\n",
    "\n",
    "Should I subclass ModifiedGeneralizedRCNN some more to redifine fast_rcnn_inference_single_image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91217f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import Boxes, Instances\n",
    "from detectron2.layers import batched_nms\n",
    "\n",
    "def fast_rcnn_inference_single_image(\n",
    "    boxes,\n",
    "    scores,\n",
    "    image_shape: Tuple[int, int],\n",
    "    score_thresh: float,\n",
    "    nms_thresh: float,\n",
    "    topk_per_image: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Single-image inference. Return bounding-box detection results by thresholding\n",
    "    on scores and applying non-maximum suppression (NMS).\n",
    "    \"\"\"\n",
    "    \n",
    "    # sanity check that all values are finite, i.e. they are not NaN, negative infinity, or infinity\n",
    "    valid_mask = torch.isfinite(boxes).all(dim=1) & torch.isfinite(scores).all(dim=1)\n",
    "    if not valid_mask.all():\n",
    "        boxes = boxes[valid_mask]\n",
    "        scores = scores[valid_mask]\n",
    "\n",
    "    scores = scores[:, :-1] # drop last class as that is background (class 80)\n",
    "    raw_scores = scores.detach().clone()    \n",
    "    \n",
    "    # total number of classes (80)\n",
    "    num_bbox_reg_classes = boxes.shape[1] // 4\n",
    "    \n",
    "    # Convert to Boxes to use the `clip` function\n",
    "    '''\n",
    "    .reshape(-1, 4) to flatten out the 80 class-specific bounding boxes per ROI \n",
    "        into 80*NUM_ROI w NUM_ROI=1000 boxes\n",
    "    '''\n",
    "    boxes = Boxes(boxes.reshape(-1, 4)) \n",
    "    '''\n",
    "    Clip (in place) the boxes by limiting x coordinates to the range [0, width]\n",
    "        and y coordinates to the range [0, height].\n",
    "    '''\n",
    "    boxes.clip(image_shape)\n",
    "    '''\n",
    "    .view(-1, num_bbox_reg_classes, 4) to convert back to NUM_ROI * 80 * 4\n",
    "    Somehow the order doesn't get mixed up lol works as expected :O\n",
    "    '''\n",
    "    boxes = boxes.tensor.view(-1, num_bbox_reg_classes, 4)\n",
    "\n",
    "    # 1. Filter results based on detection scores. It can make NMS more efficient\n",
    "    #    by filtering out low-confidence detections.\n",
    "    filter_mask = scores > score_thresh  # R x K w R = 1000, K = 80\n",
    "    \n",
    "    # R' x 2. First column contains indices of the R predictions;\n",
    "    # Second column contains indices of classes.\n",
    "    # R' <= R\n",
    "    filter_inds = filter_mask.nonzero()\n",
    "    \n",
    "    if num_bbox_reg_classes == 1:\n",
    "        boxes = boxes[filter_inds[:, 0], 0]\n",
    "    else:\n",
    "        boxes = boxes[filter_mask] # [R', 4]\n",
    "        \n",
    "    scores = scores[filter_mask] # [R']\n",
    "    raw_scores = raw_scores[filter_inds[:,0]] # [R', K] w K = 80\n",
    "    print(raw_scores.shape)\n",
    "\n",
    "    # 2. Apply NMS for each class independently.\n",
    "    keep = batched_nms(boxes, scores, filter_inds[:, 1], nms_thresh)\n",
    "    \n",
    "    if topk_per_image >= 0:\n",
    "        keep = keep[:topk_per_image]\n",
    "        # int64 tensor with the indices of the elements that \n",
    "        # have been kept by NMS, sorted in decreasing order of scores\n",
    "        \n",
    "    boxes, scores, filter_inds, raw_scores = boxes[keep], scores[keep], filter_inds[keep], raw_scores[keep]\n",
    "    print(raw_scores.shape)\n",
    "\n",
    "    # convert results into Instances\n",
    "    result = Instances(image_shape)\n",
    "    result.pred_boxes = Boxes(boxes)\n",
    "    result.scores = scores\n",
    "    result.pred_classes = filter_inds[:, 1]\n",
    "    result.raw_scores = raw_scores\n",
    "    \n",
    "    # Instances, Tensor\n",
    "    return result, filter_inds[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8623a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fast_rcnn_inference(\n",
    "            boxes,\n",
    "            scores,\n",
    "            image_shapes,\n",
    "            score_thresh=0.5,\n",
    "            nms_thresh=0.5,\n",
    "            topk_per_image=10,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd41f32",
   "metadata": {},
   "source": [
    "Only step left is post processing, which does not change any of the values but just presents the data in a different manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234ba2a",
   "metadata": {},
   "source": [
    "### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073c3ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._postprocess(pred_instances, input_img, images.image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21defd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746b314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a37c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cff6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f861226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd0dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bc63343",
   "metadata": {},
   "source": [
    "## Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300ad6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from captum.attr import (\n",
    "    GradientShap,\n",
    "    DeepLift,\n",
    "    DeepLiftShap,\n",
    "    IntegratedGradients,\n",
    "    LayerConductance,\n",
    "    NeuronConductance,\n",
    "    NoiseTunnel,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0ad802",
   "metadata": {},
   "source": [
    "## Run on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8791f99f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_   = torch.from_numpy(img).permute(2,0,1).unsqueeze(0)\n",
    "baseline = torch.zeros(input_.shape)\n",
    "baseline.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38ed2c0",
   "metadata": {},
   "source": [
    "### Need input_/baseline to be tensor, model needs to take in individual tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap(input_tensor):\n",
    "    print(input_tensor.shape)\n",
    "    outputs = model(input_tensor)\n",
    "    print(outputs)\n",
    "    return outputs[0]['instances'][selected_pred].pred_classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e836243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(wrap)\n",
    "attributions, delta = ig.attribute(input_, baseline, target=0, return_convergence_delta=True)\n",
    "print('IG Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb589e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac60718c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3dd778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca94721f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9826cbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68abc710",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GradientShap(model)\n",
    "\n",
    "# We define a distribution of baselines and draw `n_samples` from that\n",
    "# distribution in order to estimate the expectations of gradients across all baselines\n",
    "baseline_dist = torch.randn(5, 3, 480, 640) * 0.001\n",
    "attributions, delta = gs.attribute(input_, stdevs=0.09, n_samples=4, baselines=baseline_dist,\n",
    "                                   target=0, return_convergence_delta=True)\n",
    "print('GradientShap Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)\n",
    "print('Average delta per example:', torch.mean(delta.reshape(input.shape[0], -1), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeab3eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DeepLift(model)\n",
    "attributions, delta = dl.attribute(input_, baseline, target=0, return_convergence_delta=True)\n",
    "print('DeepLift Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861aadd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dl = DeepLiftShap(model)\n",
    "attributions, delta = dl.attribute(input_.float(), baseline_dist, target=0, return_convergence_delta=True)\n",
    "print('DeepLiftSHAP Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)\n",
    "print('Average delta per example:', torch.mean(delta.reshape(input.shape[0], -1), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(model)\n",
    "nt = NoiseTunnel(ig)\n",
    "attributions, delta = nt.attribute(input_, nt_type='smoothgrad', stdevs=0.02, nt_samples=4,\n",
    "      baselines=baseline, target=0, return_convergence_delta=True)\n",
    "print('IG + SmoothGrad Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)\n",
    "print('Average delta per example', torch.mean(delta.reshape(input.shape[0], -1), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da398095",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = NeuronConductance(model, model.backbone)\n",
    "attributions = nc.attribute(input_, neuron_selector=1, target=0)\n",
    "print('Neuron Attributions:', attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LayerConductance(model, model.backbone)\n",
    "attributions, delta = lc.attribute(input_, baselines=baseline, target=0, return_convergence_delta=True)\n",
    "print('Layer Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796591ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a23634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f178d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4804a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d6242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1285490b",
   "metadata": {},
   "source": [
    "# Toy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783eece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(3, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(3, 2)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        self.lin1.weight = nn.Parameter(torch.arange(-4.0, 5.0).view(3, 3))\n",
    "        self.lin1.bias = nn.Parameter(torch.zeros(1,3))\n",
    "        self.lin2.weight = nn.Parameter(torch.arange(-3.0, 3.0).view(2, 3))\n",
    "        self.lin2.bias = nn.Parameter(torch.ones(1,2))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.lin2(self.relu(self.lin1(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abcb411",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyModel()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_input = torch.rand(2, 3)\n",
    "model(toy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c19dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c7d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf326394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captum-vas",
   "language": "python",
   "name": "captum-vas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
